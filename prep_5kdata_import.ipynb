{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of prep_5kdata_import.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOE2XOca3vmLUHnxNDwf+Px",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShuYuHuang/nlp4aml/blob/master/prep_5kdata_import.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfC94Sb_XgP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "76386ac8-c6c2-4721-83a4-281a227a41d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('~/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /root/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oudCKmSmbeYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHrDgVjVb2Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_loc='/root/drive/My Drive/ColabData/'\n",
        "data_pd=pd.read_csv(file_loc+'tbrain_train_final_0610.csv')\n",
        "def get_homepage(site):\n",
        "  cnt=0\n",
        "  begin=0\n",
        "  for ee, ww in enumerate(site):\n",
        "      if ww=='/':\n",
        "        cnt+=1\n",
        "        if cnt==2:\n",
        "          begin=ee+1\n",
        "        elif cnt==3:\n",
        "          return site[begin:ee]\n",
        "  return 0\n",
        "data_pd['homepage']=data_pd['hyperlink'].apply(lambda w: get_homepage(w))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI-zWkTTfLQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "b6edfb72-b1e4-4a08-c8b1-5225493cd018"
      },
      "source": [
        "data_pd.sample(3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_ID</th>\n",
              "      <th>hyperlink</th>\n",
              "      <th>content</th>\n",
              "      <th>name</th>\n",
              "      <th>homepage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3494</th>\n",
              "      <td>3495</td>\n",
              "      <td>https://udn.com/news/story/6811/4055559</td>\n",
              "      <td>FedEx調降獲利展望 ### 省略內文 ### 已成為貨運競爭對手。</td>\n",
              "      <td>[]</td>\n",
              "      <td>udn.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1946</th>\n",
              "      <td>1947</td>\n",
              "      <td>https://www.chinatimes.com/realtimenews/201910...</td>\n",
              "      <td>台南市南化烏山風景區著名景點「龍虎寺」 ### 省略內文 ### 請與玉井偵查隊06-574...</td>\n",
              "      <td>[]</td>\n",
              "      <td>www.chinatimes.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3923</th>\n",
              "      <td>3924</td>\n",
              "      <td>https://news.mingpao.com/ins/%e6%b8%af%e8%81%9...</td>\n",
              "      <td>(12:55) prevnext 34歲男護士今年5月涉於荃灣港安醫院非禮一名女子 ### ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>news.mingpao.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      news_ID  ...            homepage\n",
              "3494     3495  ...             udn.com\n",
              "1946     1947  ...  www.chinatimes.com\n",
              "3923     3924  ...    news.mingpao.com\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8wexGijMPgM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "df2f3e83-c145-4480-af71-27af0c8f5b59"
      },
      "source": [
        "data_pd['homepage'].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "news.cnyes.com              709\n",
              "www.mirrormedia.mg          626\n",
              "www.chinatimes.com          621\n",
              "m.ctee.com.tw               530\n",
              "udn.com                     448\n",
              "news.mingpao.com            379\n",
              "news.ltn.com.tw             332\n",
              "mops.twse.com.tw            273\n",
              "technews.tw                 128\n",
              "www.businesstoday.com.tw    120\n",
              "www.ettoday.net             118\n",
              "www.hk01.com                117\n",
              "news.tvbs.com.tw            112\n",
              "sina.com.hk                  72\n",
              "www.bnext.com.tw             59\n",
              "hk.on.cc                     57\n",
              "finance.technews.tw          56\n",
              "www.cw.com.tw                45\n",
              "domestic.judicial.gov.tw     30\n",
              "www.wealth.com.tw            29\n",
              "tw.news.yahoo.com            27\n",
              "www.coolloud.org.tw          25\n",
              "www.setn.com                 16\n",
              "www.managertoday.com.tw      14\n",
              "www.hbrtaiwan.com            14\n",
              "m.ltn.com.tw                 12\n",
              "ccc.technews.tw              10\n",
              "www.fsc.gov.tw                9\n",
              "money.udn.com                 8\n",
              "ec.ltn.com.tw                 6\n",
              "www.cna.com.tw                6\n",
              "ol.mingpao.com                4\n",
              "www.nextmag.com.tw            4\n",
              "ent.ltn.com.tw                2\n",
              "news.ebc.net.tw               1\n",
              "www.nownews.com               1\n",
              "www.storm.mg                  1\n",
              "house.ettoday.net             1\n",
              "estate.ltn.com.tw             1\n",
              "Name: homepage, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7rt0so3Oz73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "outputId": "fc5f03f0-bb10-4c14-8566-bf51ab60ad00"
      },
      "source": [
        "display(data_pd[data_pd['homepage']=='estate.ltn.com.tw'][:])\n",
        "display(data_pd['hyperlink'][502])"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_ID</th>\n",
              "      <th>hyperlink</th>\n",
              "      <th>content</th>\n",
              "      <th>name</th>\n",
              "      <th>homepage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>503</td>\n",
              "      <td>http://estate.ltn.com.tw/article/6814</td>\n",
              "      <td>位於桃園市桃園區春日路的建案「銘曜高尚」近日驚傳建商落跑 ### 省略內文 ### 才能確保...</td>\n",
              "      <td>['邱俊銘', '邱水成', '邱秀芬']</td>\n",
              "      <td>estate.ltn.com.tw</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     news_ID  ...           homepage\n",
              "502      503  ...  estate.ltn.com.tw\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'http://estate.ltn.com.tw/article/6814'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR45OouyWe7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_content(ii):  \n",
        "  global data_pd\n",
        "  r = requests.get(data_pd['hyperlink'][ii])\n",
        "  # Create soup from content of request\n",
        "  c = r.content\n",
        "  soup = BeautifulSoup(c)\n",
        "  warns=False\n",
        "  try:\n",
        "    txt=\"<h1>\"+soup.title.text+\"</h1>\"\n",
        "    \n",
        "  #####################Stage 2 ###########################################\n",
        "    elif data_pd['homepage'][ii]=='www.bnext.com.tw'':\n",
        "      for aa in soup.find(\"article\",attrs={\"class\":\"main_content\"}).findAll('p'):\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.find(\"span\",attrs={\"class\":\"item\"}).text+\"</time>\" \n",
        "\n",
        "    else:\n",
        "      warns=True\n",
        "\n",
        "    txt+=\"<aml>\"+data_pd['name'][ii]+\"</aml>\"\n",
        "    if warns==False:\n",
        "      file1 = open(f\"{file_loc}5k_news_data/{data_pd['news_ID'][ii]}.txt\",\"w\")#write mode \n",
        "      file1.write(txt) \n",
        "      file1.close()\n",
        "  except:\n",
        "    file1 = open(f\"{file_loc}5k_news_data/{data_pd['news_ID'][ii]}.txt\",\"w\")#write mode \n",
        "    file1.write(\"404-error\") \n",
        "    file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPJiDuyzc9He",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def extract_content(ii):\n",
        "  global data_pd\n",
        "  r = requests.get(data_pd['hyperlink'][ii])\n",
        "  # Create soup from content of request\n",
        "  c = r.content\n",
        "  soup = BeautifulSoup(c)\n",
        "  warns=False\n",
        "  homepage=data_pd['homepage'][ii]\n",
        "  try:\n",
        "    txt=\"<h1>\"+soup.title.text+\"</h1>\"\n",
        "    #####################Dat 1 progress ###########################################\n",
        "    if data_pd['homepage'][ii]=='news.cnyes.com':\n",
        "      txt+=soup.find('section').text\n",
        "      txt+=\"<time>\"+soup.find('time').text+\"</time>\"\n",
        "  \n",
        "    elif data_pd['homepage'][ii]=='www.mirrormedia.mg':\n",
        "      for aa in soup.find_all('p'):\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.find('div', attrs={\"class\":\"date\"}).text+\"</time>\"\n",
        "\n",
        "    elif data_pd['homepage'][ii]==\"www.chinatimes.com\":\n",
        "      txt+=soup.find('div', attrs={\"class\":\"article-body\"}).text\n",
        "      txt+=\"<time>\"+soup.find('time')['datetime']+\"</time>\"\n",
        "\n",
        "    elif data_pd['homepage'][ii]==\"m.ctee.com.tw\":\n",
        "      txt+=soup.find('div', attrs={\"class\":\"entry-content\"}).text\n",
        "      txt+=\"<time>\"+soup.find('div',attrs={\"class\":\"post-meta-date\"}).text+\"</time>\"\n",
        "\n",
        "    elif data_pd['homepage'][ii]==\"udn.com\":\n",
        "      txt+=soup.find('section',attrs={'class':'article-content__editor '}).text\n",
        "      txt+=\"<time>\"+soup.find('time').text+\"</time>\"\n",
        "\n",
        "    elif data_pd['homepage'][ii]==\"news.mingpao.com\":\n",
        "      txt+=soup.find('article').text\n",
        "      txt+=\"<time>\"+soup.find('div',attrs={'itemprop':'datePublished'}).text+\"</time>\"\n",
        "\n",
        "    elif data_pd['homepage'][ii]==\"news.ltn.com.tw\":\n",
        "      for aa in soup.find('div',attrs={'class':'text boxTitle boxText'}).find_all('p'):\n",
        "        if not aa.has_attr('class'):\n",
        "          txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.find('span',attrs={'class':'time'}).text+\"</time>\"\n",
        "\n",
        "    elif data_pd['homepage'][ii]==\"mops.twse.com.tw\":\n",
        "      tmp=soup.find_all('td', attrs={\"style\":\"text-align:left !important;\"})\n",
        "      txt=\"<h1>\"+tmp[6].text+\"</h1>\"+tmp[9].text\n",
        "      txt+=\"<time>\"+tmp[1].text+\"</time>\"\n",
        "\n",
        "    elif  data_pd['homepage'][ii]==\"technews.tw\":\n",
        "      for aa in soup.find('div',class_=\"indent\").find_all('p')[:-1]:\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.find_all('span', attrs={\"body\"})[1].text+\"</time>\"\n",
        "\n",
        "    elif  data_pd['homepage'][ii]==\"www.businesstoday.com.tw\":\n",
        "      txt+=soup.find('div', attrs={\"itemprop\":\"articleBody\"}).text  \n",
        "      txt+=\"<time>\"+soup.find(\"meta\",attrs={\"property\":\"article:published_time\"})['content']+\"</time>\"\n",
        "\n",
        "    elif  data_pd['homepage'][ii]==\"www.ettoday.net\":\n",
        "      for aa in soup.find(\"div\",attrs={\"class\":\"content-container\"}).find_all(\"p\")[3:]:\n",
        "        if aa.text!='請繼續往下閱讀...':\n",
        "          txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.find(\"time\",attrs={\"class\":\"header-time\"})['datetime']+\"</time>\" \n",
        "\n",
        "    elif data_pd['homepage'][ii]=='www.hk01.com':\n",
        "      for aa in soup.findAll(\"p\")[2:]:\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.time.text+\"</time>\" \n",
        "\n",
        "    elif data_pd['homepage'][ii]=='news.tvbs.com.tw':\n",
        "      for aa in AA.findAll('br')[::2]:\n",
        "        txt+=aa.previous_element\n",
        "      txt+=\"<time>\"+soup.find(\"meta\",attrs={\"name\":\"pubdate\"})[\"content\"]+\"</time>\" \n",
        "\n",
        "    elif data_pd['homepage'][ii]=='sina.com.hk':\n",
        "      for aa in soup.find(\"div\",attrs={\"class\":\"news-body\"}).findAll(\"p\")[:-5]:\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.find(\"div\",attrs={\"class\":\"news-datetime\"}).text+\"</time>\" \n",
        "    #####################Day 2 progress###########################################\n",
        "    if homepage=='www.bnext.com.tw':\n",
        "      for aa in soup.find(\"article\",attrs={\"class\":\"main_content\"}).findAll('p'):\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.find(\"span\",attrs={\"class\":\"item\"}).text+\"</time>\" \n",
        "\n",
        "    elif homepage=='hk.on.cc':\n",
        "      for aa in soup.findAll(\"div\",attrs={\"class\":\"paragraph\"}):\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.find(\"span\",attrs={\"class\":\"datetime\"}).text+\"</time>\" \n",
        "      \n",
        "    elif homepage=='finance.technews.tw':\n",
        "      for aa in soup.find(\"div\",attrs={\"class\":\"indent\"}).findAll(\"p\"):\n",
        "        txt+=aa.text\n",
        "      for aa in soup.find_all(\"span\",attrs={\"class\":\"body\"}):\n",
        "        if aa.findPrevious(\"span\",class_=\"head\").text==\"發布日期\":\n",
        "          txt+=\"<time>\"+aa.text+\"</time>\" \n",
        "\n",
        "    elif homepage=='www.cw.com.tw':\n",
        "      txt=\"<h1>\"+soup.h1.text+\"</h1>\"\n",
        "      for aa in soup.findAll('p')[3:-11]:\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.time.text+\"</time>\" \n",
        "\n",
        "    elif homepage=='www.wealth.com.tw':\n",
        "      txt+=soup.find(\"div\",attrs={\"itemprop\":\"articleBody\"}).text\n",
        "      txt+=\"<time>\"+soup.find(\"meta\",attrs={\"property\":\"article:published_time\"})['content']+\"</time>\"\n",
        "\n",
        "    elif homepage=='tw.news.yahoo.com':\n",
        "      for aa in soup.find('div',attrs={\"class\":\"caas-body\"}).findAll('p')[:-6]:\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.time.text+\"</time>\" \n",
        "\n",
        "    elif homepage=='www.coolloud.org.tw':\n",
        "      txt+=soup.find('div',attrs={\"class\":\"field field-name-body field-type-text-with-summary field-label-hidden\"}).text\n",
        "      txt+=\"<time>\"+soup.find(\"span\",attrs={\"class\":\"date-display-single\"})+\"</time>\" \n",
        "    \n",
        "    elif homepage=='www.setn.com':\n",
        "      txt+=soup.article.text\n",
        "      txt+=\"<time>\"+soup.find(\"time\",class_=\"page-date\").text+\"</time>\" \n",
        "\n",
        "    elif homepage=='www.managertoday.com.tw':\n",
        "      for aa in soup.article.findAll(\"p\"):\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.article.find(\"time\").text+\"</time>\" \n",
        "\n",
        "    elif homepage=='www.hbrtaiwan.com':\n",
        "      for aa in soup.find(\"div\",attrs={\"class\":\"article article-first-row premium\"}).findAll(\"p\"):\n",
        "        txt+=aa.text\n",
        "      matches= re.compile(r\"datePublished\\\":\\ \\\".*\\\"\").finditer(soup.prettify())\n",
        "      for mm in matches:\n",
        "        txt+=\"<time>\"+mm.group(0)[17:-1]+\"</time>\"\n",
        "        break\n",
        "\n",
        "    elif homepage=='m.ltn.com.tw':\n",
        "      for aa in soup.find(\"div\",attrs={\"data-desc\":\"內文\"}).findAll(\"p\"):\n",
        "        if not aa.has_attr('class'):\n",
        "          txt+=aa.text\n",
        "      matches= re.compile(r\"datePublished\\\":\\ \\\".*\\\"\").finditer(soup.prettify())\n",
        "      for mm in matches:\n",
        "        txt+=\"<time>\"+mm.group(0)[17:-1]+\"</time>\"\n",
        "        break\n",
        "\n",
        "    elif homepage=='ccc.technews.tw':\n",
        "      for aa in soup.find(\"div\",attrs={\"class\":\"indent\"}).findAll(\"p\"):\n",
        "        if not aa.has_attr('class'):\n",
        "          txt+=aa.text\n",
        "      for aa in soup.find_all(\"span\",attrs={\"class\":\"body\"}):\n",
        "        if aa.findPrevious(\"span\",class_=\"head\").text==\"發布日期\":\n",
        "          txt+=\"<time>\"+aa.text+\"</time>\"\n",
        "          break\n",
        "        \n",
        "    elif homepage=='www.fsc.gov.tw':\n",
        "      for aa in soup.find(\"div\",attrs={\"class\":\"indent\"}).findAll(\"p\"):\n",
        "        if not aa.has_attr('class'):\n",
        "          txt+=aa.text\n",
        "      txt=\"</time>\"+soup.find(\"div\",class_=\"contentdate\").txt+\"</time>\"\n",
        "\n",
        "    elif homepage=='money.udn.com':\n",
        "      for aa in soup.find(\"div\",attrs={\"id\":\"article_body\"}).findAll(\"p\"):\n",
        "          txt+=aa.text\n",
        "      matches= re.compile(r\"datePublished\\\":\\ \\\".*\\\"\").finditer(soup.prettify())\n",
        "      for mm in matches:\n",
        "        txt+=\"<time>\"+mm.group(0)[17:-1]+\"</time>\"\n",
        "        break\n",
        "\n",
        "    elif homepage=='ec.ltn.com.tw':\n",
        "      for aa in soup.find(\"div\",attrs={\"data-desc\":\"內文\"}).findAll(\"p\"):\n",
        "        if not aa.has_attr('class'):\n",
        "          txt+=aa.text\n",
        "      matches= re.compile(r\"datePublished\\\":\\ \\\".*\\\"\").finditer(soup.prettify())\n",
        "      for mm in matches:\n",
        "        txt+=\"<time>\"+mm.group(0)[17:-1]+\"</time>\"\n",
        "        break\n",
        "    \n",
        "    elif homepage=='www.cna.com.tw':\n",
        "      for aa in soup.findAll(\"p\"):\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.find(\"div\",class_=\"updatetime\").span.text+\"</time>\"\n",
        "    \n",
        "    elif homepage=='ol.mingpao.com':\n",
        "      for aa in soup.findAll(\"p\"):\n",
        "        txt+=aa.text\n",
        "      txt+=\"<time>\"+soup.h5.text[5:]+\"</time>\"\n",
        "\n",
        "    elif homepage=='www.nextmag.com.tw':\n",
        "      for aa in soup.find(\"div\",class_=\"article-content\").findAll(\"p\")[1:]:\n",
        "        txt+=aa.text\n",
        "      matches= re.compile(r\"datePublished\\\":\\ \\\".*\\\"\").finditer(soup.prettify())\n",
        "      for mm in matches:\n",
        "        txt+=\"<time>\"+mm.group(0)[17:-1]+\"</time>\"\n",
        "        break\n",
        "\n",
        "    elif homepage=='ent.ltn.com.tw':\n",
        "      for aa in soup.findAll(\"p\"):\n",
        "        if not aa.has_attr('class'):\n",
        "          txt+=aa.text\n",
        "      matches= re.compile(r\"datePublished\\\":\\ \\\".*\\\"\").finditer(soup.prettify())\n",
        "      for mm in matches:\n",
        "        txt+=\"<time>\"+mm.group(0)[17:-1]+\"</time>\"\n",
        "        break\n",
        "    \n",
        "    elif homepage=='news.ebc.net.tw':\n",
        "      for aa in soup.find(\"div\",attrs={\"class\":\"raw-style\"}).findAll(\"p\"):\n",
        "        if not aa.has_attr('class'):\n",
        "          txt+=aa.text\n",
        "      matches= re.compile(r\"datePublished\\\":\\ \\\".*\\\"\").finditer(soup.prettify())\n",
        "      for mm in matches:\n",
        "        txt+=\"<time>\"+mm.group(0)[17:-1]+\"</time>\"\n",
        "        break\n",
        "    \n",
        "    elif homepage=='www.nownews.com':\n",
        "      txt+=soup.find(\"div\",attrs={\"itemprop\":\"articleBody\"}).text\n",
        "      matches= re.compile(r\"datePublished\\\":\\ \\\".*\\\"\").finditer(soup.prettify())\n",
        "      for mm in matches:\n",
        "        txt+=\"<time>\"+mm.group(0)[17:-1]+\"</time>\"\n",
        "        break\n",
        "    \n",
        "    elif homepage=='www.storm.mg':\n",
        "      txt+=soup.find(\"div\",attrs={\"itemprop\":\"articleBody\"}).text\n",
        "      txt+=soup.find(\"meta\",attrs={\"name\":\"publishedTime\"})[\"content\"]\n",
        "    \n",
        "        \n",
        "    elif homepage=='house.ettoday.net':\n",
        "      txt+=soup.find(\"div\",attrs={\"itemprop\":\"articleBody\"}).text\n",
        "      txt+=soup.find(\"meta\",attrs={\"name\":\"pubdate\"})[\"content\"]\n",
        "\n",
        "    elif homepage=='estate.ltn.com.tw':\n",
        "      txt+=soup.find(\"div\",attrs={\"itemprop\":\"articleBody\"}).text\n",
        "      txt+=soup.find(\"meta\",attrs={\"name\":\"pubdate\"})[\"content\"]\n",
        "\n",
        "    else:\n",
        "      warns=True\n",
        "\n",
        "    txt+=\"<aml>\"+data_pd['name'][ii]+\"</aml>\"\n",
        "    if warns==False:\n",
        "      file1 = open(f\"{file_loc}5k_news_data/{data_pd['news_ID'][ii]}.txt\",\"w\")#write mode \n",
        "      file1.write(txt) \n",
        "      file1.close()\n",
        "  except:\n",
        "    file1 = open(f\"{file_loc}5k_news_data/{data_pd['news_ID'][ii]}.txt\",\"w\")#write mode \n",
        "    file1.write(\"404-error\") \n",
        "    file1.close()"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xSFhuS3MQ2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "for ii in data_pd.index[24:]:\n",
        "  if ii>0:\n",
        "    if data_pd['homepage'][ii]==data_pd['homepage'][ii-1]:\n",
        "      time.sleep(1)\n",
        "  extract_content(ii)"
      ],
      "execution_count": 231,
      "outputs": []
    }
  ]
}